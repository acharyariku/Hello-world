{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EluvioDSChallenge.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMQmttur51aRUzCYYmTNeR6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/acharyariku/Hello-world/blob/master/EluvioDSChallenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV7at7LHZM6c"
      },
      "source": [
        "**Define a problem:**\n",
        "Through pandas to read the csv files, I just found the file contains 8 columns(509236*8): [\"time_created\"、\"date_created\"、\"up_votes\"、\"down_votes\"、\"title\"、\"over_18\"、\"author\" and \"category\"].\n",
        "\n",
        "datasets\n",
        "\n",
        "After simple checking, the column called \"down_votes\" are found all \"0\" and the column named \"category\" are all shown as \"worldnews\", so those two features are useless, and just drop them. So there are 6 columns left.\n",
        "\n",
        "Since there is no clear label, I want to set my own label based on the features value. In my opinion, whether the news is attracting or not can be a label, and by the virtue of \"up_votes\" values, the news can be easily split to be two parts(Ones are attracting news and the others are not attracting news). And this is very intuitive and reasonable since there should be more \"up_votes\" if the news is attracting.\n",
        "\n",
        "So which columns can be features affecting the label(attracing news or not)? Obviously, the \"title\" should be one feature, since if the title is attracting, there should be more people to read and more probably to like it. And the \"over_18\" should also be one factor since age will sometimes or to some extent determine their interest. And \"author\" have to be one factor since some famous actor will be more likely to get likeness. And the time columns [time_created, date_created] should have some influences on the label but I do not think the effect is huge, so here I just drop them.\n",
        "\n",
        "And there are 85838 authors, which is too much if we do one-hot encoding, so here let us just dropped it first.\n",
        "\n",
        "After clarification, here is a classification problem, the label is whether a news is attracting or not(based on \"up_votes\") and the features are [\"title\", \"over_18\"]. In this problem, I just use the feature ['title'] to simplify the problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF5qUWeiCsiO",
        "outputId": "a9e1924c-68a6-4a36-8d02-ade8690f4836"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BpHiMo0CuYO"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8PrVQYnDAPY"
      },
      "source": [
        "path = \"drive/My Drive/Eluvio_DS_Challenge.csv\"\n",
        "df = pd.read_csv(path)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "FGz_6MmoFlhl",
        "outputId": "5f69a771-77eb-497f-e4f0-2585c8d4a533"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_created</th>\n",
              "      <th>date_created</th>\n",
              "      <th>up_votes</th>\n",
              "      <th>down_votes</th>\n",
              "      <th>title</th>\n",
              "      <th>over_18</th>\n",
              "      <th>author</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1201232046</td>\n",
              "      <td>2008-01-25</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>Scores killed in Pakistan clashes</td>\n",
              "      <td>False</td>\n",
              "      <td>polar</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1201232075</td>\n",
              "      <td>2008-01-25</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>Japan resumes refuelling mission</td>\n",
              "      <td>False</td>\n",
              "      <td>polar</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1201232523</td>\n",
              "      <td>2008-01-25</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>US presses Egypt on Gaza border</td>\n",
              "      <td>False</td>\n",
              "      <td>polar</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1201233290</td>\n",
              "      <td>2008-01-25</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Jump-start economy: Give health care to all</td>\n",
              "      <td>False</td>\n",
              "      <td>fadi420</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1201274720</td>\n",
              "      <td>2008-01-25</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>Council of Europe bashes EU&amp;UN terror blacklist</td>\n",
              "      <td>False</td>\n",
              "      <td>mhermans</td>\n",
              "      <td>worldnews</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   time_created date_created  up_votes  ...  over_18    author   category\n",
              "0    1201232046   2008-01-25         3  ...    False     polar  worldnews\n",
              "1    1201232075   2008-01-25         2  ...    False     polar  worldnews\n",
              "2    1201232523   2008-01-25         3  ...    False     polar  worldnews\n",
              "3    1201233290   2008-01-25         1  ...    False   fadi420  worldnews\n",
              "4    1201274720   2008-01-25         4  ...    False  mhermans  worldnews\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_R2G3yxFs6F",
        "outputId": "3d5cd804-bb97-4ccf-b213-2045970ef84b"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "509236"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Cw2WML2FyCn",
        "outputId": "a735fc48-b9dd-40df-fd92-3f875a6cdccd"
      },
      "source": [
        "print(sum(df['category'] == \"worldnews\"))\n",
        "print(sum(df[\"down_votes\"] == 0))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "509236\n",
            "509236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcH2buQNF1yJ"
      },
      "source": [
        "df = df.drop(\"category\", axis = 1)\n",
        "df = df.drop(\"down_votes\", axis = 1)\n",
        "df = df.drop(\"time_created\", axis = 1)\n",
        "df = df.drop(\"date_created\", axis = 1)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "TEw870AVF4fa",
        "outputId": "537d2a71-6ca5-47b4-b661-c01b713cf0c8"
      },
      "source": [
        "df.head()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>up_votes</th>\n",
              "      <th>title</th>\n",
              "      <th>over_18</th>\n",
              "      <th>author</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>Scores killed in Pakistan clashes</td>\n",
              "      <td>False</td>\n",
              "      <td>polar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Japan resumes refuelling mission</td>\n",
              "      <td>False</td>\n",
              "      <td>polar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>US presses Egypt on Gaza border</td>\n",
              "      <td>False</td>\n",
              "      <td>polar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Jump-start economy: Give health care to all</td>\n",
              "      <td>False</td>\n",
              "      <td>fadi420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Council of Europe bashes EU&amp;UN terror blacklist</td>\n",
              "      <td>False</td>\n",
              "      <td>mhermans</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   up_votes                                            title  over_18    author\n",
              "0         3                Scores killed in Pakistan clashes    False     polar\n",
              "1         2                 Japan resumes refuelling mission    False     polar\n",
              "2         3                  US presses Egypt on Gaza border    False     polar\n",
              "3         1     Jump-start economy: Give health care to all     False   fadi420\n",
              "4         4  Council of Europe bashes EU&UN terror blacklist    False  mhermans"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytff4QkJGC-a",
        "outputId": "5c917d31-bfff-4b3a-eb57-c64565b96271"
      },
      "source": [
        "\n",
        "len(df['author'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "509236"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ujdp5EEOI5Y7",
        "outputId": "5421cb43-ed11-4d7d-c2bf-164eab7782c7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7y5wFLYJFsU"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DA3VbhlJLB2"
      },
      "source": [
        "# To get the stems of words in a sentence.\n",
        "def tokenize_and_stem(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    \n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "    return stems\n",
        "\n",
        "# To get the words themself in a sentence.\n",
        "def tokenize_only(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    return filtered_tokens"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sEad3UDJOGp"
      },
      "source": [
        "#lowercase\n",
        "title = df.title.str.lower()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3fUS5QKJRIh"
      },
      "source": [
        "# Get full stems and tokens to build vocabulary\n",
        "def tokenized_stemmed(title):\n",
        "    totalvocab_stemmed = []\n",
        "    totalvocab_tokenized = []\n",
        "    for i in title:\n",
        "        allwords_stemmed = tokenize_and_stem(i) \n",
        "        totalvocab_stemmed.extend(allwords_stemmed) \n",
        "\n",
        "        allwords_tokenized = tokenize_only(i)\n",
        "        totalvocab_tokenized.extend(allwords_tokenized)\n",
        "    return totalvocab_stemmed, totalvocab_tokenized"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e0SspkHJUWR"
      },
      "source": [
        "totalvocab_stemmed, totalvocab_tokenized = tokenized_stemmed(title)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsVRyeBXJjFP",
        "outputId": "1091dcf9-e876-40ab-bddd-8e4697afe15e"
      },
      "source": [
        "print(len(totalvocab_stemmed))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7194561\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wosvw7r1JutB"
      },
      "source": [
        "pickle.dump((totalvocab_stemmed, totalvocab_tokenized), open(\"drive/My Drive/stem_token.pkl\", \"wb\" ))\n",
        "totalvocab_stemmed, totalvocab_tokenized = pickle.load(open(\"drive/My Drive/stem_token.pkl\", \"rb\" ))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFlKbfUUQOZR"
      },
      "source": [
        "# Rule out repetitions of stem-token pairs\n",
        "totalvocab = zip(totalvocab_stemmed, totalvocab_tokenized)\n",
        "totalvocab = list(set(totalvocab))\n",
        "totalvocab_stemmed, totalvocab_tokenized = zip(*totalvocab)\n",
        "\n",
        "pickle.dump((totalvocab_stemmed, totalvocab_tokenized), open(\"drive/My Drive/stem_token.pkl\", \"wb\" ))\n",
        "\n",
        "totalvocab_stemmed, totalvocab_tokenized = pickle.load(open(\"drive/My Drive/stem_token.pkl\", \"rb\" ))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwX7E8jGKBEL",
        "outputId": "cb928431-9625-4743-b77a-86fd1391fdd4"
      },
      "source": [
        "print(len(totalvocab_stemmed))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "115041\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plTJasmIKEQE"
      },
      "source": [
        "#stem-token vocabulary\n",
        "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
        "\n",
        "pickle.dump(vocab_frame, open('drive/My Drive/vocab_frame.pkl','wb'))\n",
        "\n",
        "vocab_frame = pickle.load(open('drive/My Drive/vocab_frame.pkl','rb'))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EgcbvrCKIGX"
      },
      "source": [
        "# Build stopwords set. Combine two common set.\n",
        "import sklearn.feature_extraction.text as text\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "my_stop_words = text.ENGLISH_STOP_WORDS.union(stopwords)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hdr4NjsjKU_m",
        "outputId": "bc951736-cc32-4467-c4d9-bcd397ee89df"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df =10**-3 ,analyzer = 'word', max_features=len(set(totalvocab_stemmed)), stop_words=my_stop_words, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(title)\n",
        "\n",
        "print(tfidf_matrix.shape)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'s\", 'abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'doe', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', \"n't\", 'need', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sha', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'wo', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(509236, 1814)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8q_rBvVTd1D"
      },
      "source": [
        "\n",
        "pickle.dump(tfidf_matrix, open(\"drive/My Drive/tfidf_matrix.pkl\", \"wb\" ))\n",
        "pickle.dump(tfidf_vectorizer, open( \"drive/My Drive/tfidf_vectorizer.pkl\", \"wb\" ))\n",
        "\n",
        "tfidf_matrix = pickle.load(open(\"drive/My Drive/tfidf_matrix.pkl\", \"rb\" ))\n",
        "tfidf_vectorizer = pickle.load(open(\"drive/My Drive/tfidf_vectorizer.pkl\", \"rb\" ))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leyW-k_kTmtA",
        "outputId": "bca2ae5e-ae96-431a-e5a0-61bd55c6ec02"
      },
      "source": [
        "tfidf_matrix"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<509236x1814 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 3565328 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SecRRCMNTqmF"
      },
      "source": [
        "thre = np.quantile(df['up_votes'], 0.8)\n",
        "y = [1 if i > thre else 0 for i in df['up_votes']]\n",
        "y = np.array(y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, y, test_size = 0.2, shuffle = True, random_state = 42)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGQyAW9qTuEm",
        "outputId": "cf933674-cb0f-4818-b4ea-9159e270539f"
      },
      "source": [
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, y_train)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJgdtPowTwzz",
        "outputId": "9ea6f2d3-66f4-47f5-86a4-3cbb1b1f4d74"
      },
      "source": [
        "y_predict = clf.predict(X_test)\n",
        "clf.score(X_test, y_test)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8050624459979577"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUmiw2FiTyOP",
        "outputId": "60a00cc7-09fc-488b-a2b3-7f037de7e222"
      },
      "source": [
        "print(classification_report(y_test, y_predict))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      1.00      0.89     81988\n",
            "           1       0.56      0.00      0.00     19860\n",
            "\n",
            "    accuracy                           0.81    101848\n",
            "   macro avg       0.68      0.50      0.45    101848\n",
            "weighted avg       0.76      0.81      0.72    101848\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o02UJobGT0sh"
      },
      "source": [
        "LR = LogisticRegression(C=1.0, penalty='l2', tol=0.01)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIwo3bQ6T4NX",
        "outputId": "22d76bd6-50a1-48d3-eb4c-b70c012b7ca6"
      },
      "source": [
        "LR.fit(X_train, y_train)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.01, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBw-55-nT8MD",
        "outputId": "a1566a5d-5334-47b2-ecea-9bb9edb7a674"
      },
      "source": [
        "y_predict = LR.predict(X_test)\n",
        "LR.score(X_test, y_test)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8061817610556908"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBvH3x-oT9Fm",
        "outputId": "37f7521e-53bb-4baf-8e1a-66dfbcbd76d4"
      },
      "source": [
        "print(classification_report(y_test, y_predict))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.99      0.89     81988\n",
            "           1       0.54      0.04      0.07     19860\n",
            "\n",
            "    accuracy                           0.81    101848\n",
            "   macro avg       0.68      0.52      0.48    101848\n",
            "weighted avg       0.76      0.81      0.73    101848\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13b6-3VhT_fQ",
        "outputId": "5ca7db20-7cb0-4203-bef9-2b472f342934"
      },
      "source": [
        "gbdt = GradientBoostingClassifier()\n",
        "gbdt.fit(X_train, y_train)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
              "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
              "                           max_features=None, max_leaf_nodes=None,\n",
              "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                           min_samples_leaf=1, min_samples_split=2,\n",
              "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                           n_iter_no_change=None, presort='deprecated',\n",
              "                           random_state=None, subsample=1.0, tol=0.0001,\n",
              "                           validation_fraction=0.1, verbose=0,\n",
              "                           warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRxhRxSBUBwU",
        "outputId": "53c9c16c-fbb2-4d8e-80b8-e87a0bca48bf"
      },
      "source": [
        "y_predict = gbdt.predict(X_test)\n",
        "gbdt.score(X_test, y_test)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8053962768046501"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vYG9ijBUEMN",
        "outputId": "17872930-32f4-4b3a-edcb-292f65cd30eb"
      },
      "source": [
        "print(classification_report(y_test, y_predict))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      1.00      0.89     81988\n",
            "           1       0.73      0.00      0.01     19860\n",
            "\n",
            "    accuracy                           0.81    101848\n",
            "   macro avg       0.77      0.50      0.45    101848\n",
            "weighted avg       0.79      0.81      0.72    101848\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtXCU3NzUGXh",
        "outputId": "e556183c-2ccf-419e-f98c-7d907a04ea6d"
      },
      "source": [
        "rfc = RandomForestClassifier(n_jobs = -1, max_features = 'sqrt', n_estimators = 10, oob_score = True)\n",
        "rfc.fit(X_train, y_train)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:523: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable oob estimates.\n",
            "  warn(\"Some inputs do not have OOB scores. \"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:528: RuntimeWarning: invalid value encountered in true_divide\n",
            "  predictions[k].sum(axis=1)[:, np.newaxis])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='sqrt',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=-1,\n",
              "                       oob_score=True, random_state=None, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXnxNIs_UItx",
        "outputId": "692b100e-53d7-4be5-912e-62a352cd1d8d"
      },
      "source": [
        "y_predict = rfc.predict(X_test)\n",
        "rfc.score(X_test, y_test)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7933685492105883"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oTQ5tCcUMRn",
        "outputId": "05e5dede-d121-440c-d6f0-ac99931b6d7f"
      },
      "source": [
        "print(classification_report(y_test, y_predict))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.97      0.88     81988\n",
            "           1       0.32      0.05      0.09     19860\n",
            "\n",
            "    accuracy                           0.79    101848\n",
            "   macro avg       0.56      0.51      0.49    101848\n",
            "weighted avg       0.71      0.79      0.73    101848\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiVQpH55UOOh"
      },
      "source": [
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maoD9cOrUQyU"
      },
      "source": [
        "xgb = XGBClassifier(\n",
        " learning_rate =0.1,\n",
        " n_estimators=1000,\n",
        " max_depth=5,\n",
        " min_child_weight=1,\n",
        " gamma=0,\n",
        " subsample=0.8,\n",
        " colsample_bytree=0.8,\n",
        " objective= 'binary:logistic',\n",
        " nthread=4,\n",
        " scale_pos_weight=1,\n",
        " seed=27)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZMjgZXfUTMO",
        "outputId": "81f39f91-562c-49e3-9283-53a56af9e0f9"
      },
      "source": [
        "xgb.fit(X_train, y_train)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
              "              min_child_weight=1, missing=None, n_estimators=1000, n_jobs=1,\n",
              "              nthread=4, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27,\n",
              "              silent=None, subsample=0.8, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyu9BGDPUVlX"
      },
      "source": [
        "y_predict = xgb.predict(X_test)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63DMJtb4UXWs",
        "outputId": "85200449-3d4b-4a29-cb1d-3bac0424abb6"
      },
      "source": [
        "xgb.score(X_test, y_test)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8062406723745189"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT-F2rc5UZf4",
        "outputId": "1838f2ff-0248-4085-f19d-1a8cfd27684c"
      },
      "source": [
        "print(classification_report(y_test, y_predict))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.99      0.89     81988\n",
            "           1       0.54      0.04      0.08     19860\n",
            "\n",
            "    accuracy                           0.81    101848\n",
            "   macro avg       0.68      0.52      0.48    101848\n",
            "weighted avg       0.76      0.81      0.73    101848\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}